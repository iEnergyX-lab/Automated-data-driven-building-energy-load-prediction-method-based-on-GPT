from AutoPrediction import PromptingFunctions
from AutoPrediction import CodeInterpreter
from AutoPrediction import AutoCoding
from AutoPrediction import CodeSelfCorrection
import copy

def AutoModelDeployment(X, best_messages, GPT_model = "gpt-3.5-turbo", api_key = None, organization_id = None, temperature = 0):
    """Automated model deployment based GPT.
    This function utilizes GPT to generate Python codes of model deployment.
    
    Inputs
    ----------
    X: Input list for the prompting function. There are 5 elements in X:
      X[0]: Name of a file that stores historical data.
      X[1]: Name of a file that stores real-time data.
      X[2]: Building energy load that needs to be predicted.
      X[3]: Data normalization method.
      X[4]: Model interpretation method.
    best_messages: Best chat between GPT and computers in the step of automated model training.
    GPT_model: Name of the GPT model.
    api_key: Your API key of GPT.
    organization_id: Your organization id of GPT.
    temperature: Control the creativity of the responses generated by GPT. Higher values result in more randomness, while lower values make responses more focused and deterministic.

    Outputs
    ----------
    result: Outputs of GPT-based automated model deployment. There are 8 outputs:
      total response time of GPT: Total response time of GPT (s).
      total completion tokens of GPT: Total completion tokens of GPT.
      total prompt tokens of GPT: Total prompt tokens of GPT.
      all messages of GPT: All chats between GPT and users in the step of automted model deployment.
      code: Code offered by GPT.
      code run time: Code run time (s).
      model accuracy: Model accuracy.
      n iterations: Number of iterations for code correction.
    
    """
    
    prompt = PromptingFunctions.PromptingFunctionForModelDeployment(X)
    best_messages = [best_messages[0], best_messages[-1]]
    coding_returncode, coding_output = AutoCoding.GPTCoding(prompt = prompt, memory = best_messages, GPT_model = GPT_model, api_key = api_key, organization_id = organization_id, temperature = temperature)
    
    if coding_returncode == 1: #GPT cannot response (returncode = 2).
        print("----------------------------------")
        print("Sorry! GPT cannot provie the code.")
        print("----------------------------------")
        
        result = {"total response time of GPT": coding_output["response_time"],
                  "total completion tokens of GPT": coding_output["completion_tokens"],
                  "total prompt tokens of GPT": coding_output["prompt_tokens"],
                  "all messages of GPT": [coding_output["message"]],
                  "code": coding_output["code"],
                  "code run time": None,
                  "predicted load": None,
                  "n iterations": None}
                
        return 2, result
    
    else: #GPT responses successfully.
        total_response_time = coding_output["response_time"]
        total_completion_tokens = coding_output["completion_tokens"]
        total_prompt_tokens = coding_output["prompt_tokens"]
        all_message = [copy.deepcopy(coding_output["message"])]
        code = coding_output["code"]
        interpreter_returncode, interpreter_output = CodeInterpreter.CodeInterpreterModelDeployment(code)
        
        if interpreter_returncode == 0: #The code runs successfully (returncode = 0).  
            print("----------------------------------")
            print("Congratulation! The model has been deployed!")
            print("----------------------------------")
            
            result = {"total response time of GPT": total_response_time,
                      "total completion tokens of GPT": total_completion_tokens,
                      "total prompt tokens of GPT": total_prompt_tokens,
                      "all messages of GPT": all_message,
                      "code": code,
                      "code run time": interpreter_output["run_time_code"],
                      "predicted load": interpreter_output["predicted_load"],
                      "n iterations": None}
            
            return 0, result
            
        elif interpreter_returncode == 1: #The code needs to be corrected.
            print("----------------------------------")
            print("Sorry! The code need to be corrected.")
            print("----------------------------------")
            
            error_message = interpreter_output["error_message"]
            memory = coding_output["message"]
            selfCorrection_returncode, selfCorrection_output = CodeSelfCorrection.SelfCorrectionForModelDeployment(memory = memory, error_message = error_message, GPT_model = GPT_model, api_key = api_key, organization_id = organization_id, temperature = temperature)
            
            code = selfCorrection_output["corrected_code"]
            predicted_load = selfCorrection_output["predicted_load"]
            n_iterations = selfCorrection_output["n_iterations"]
            message = selfCorrection_output["message"]
            all_message = all_message+copy.deepcopy(message)
            total_completion_tokens = total_completion_tokens+selfCorrection_output["total_completion_tokens"]
            total_prompt_tokens = total_prompt_tokens+selfCorrection_output["total_prompt_tokens"]
            total_response_time = total_response_time+selfCorrection_output["total_response_time"]
            total_run_time_code = selfCorrection_output["total_run_time_code"]
            
            if selfCorrection_returncode == 0: #The code is corrected successfully (returncode = 0).
                print("----------------------------------")
                print("Congratulation! The model has been deployed!")
                print("----------------------------------")

                result = {"total response time of GPT": total_response_time,
                          "total completion tokens of GPT": total_completion_tokens,
                          "total prompt tokens of GPT": total_prompt_tokens,
                          "all messages of GPT": all_message,
                          "code": code,
                          "code run time": total_run_time_code,
                          "predicted load": predicted_load,
                          "n iterations": n_iterations}
                
                return 0, result
            
            elif selfCorrection_returncode == 2: #GPT cannot response (returncode = 2).
                print("----------------------------------")
                print("Sorry! GPT cannot provie the code.")
                print("----------------------------------")
                
                result = {"total response time of GPT": total_response_time,
                          "total completion tokens of GPT": total_completion_tokens,
                          "total prompt tokens of GPT": total_prompt_tokens,
                          "all messages of GPT": all_message,
                          "code": code,
                          "code run time": total_run_time_code,
                          "predicted load": predicted_load,
                          "n iterations": n_iterations}
                
                return 2, result   
            
            else: #GPT cannot correct the code (returncode = 1).
                print("----------------------------------")
                print("Sorry! The code cannot be corrected.")         
                print("----------------------------------")
                
                result = {"total response time of GPT": total_response_time,
                          "total completion tokens of GPT": total_completion_tokens,
                          "total prompt tokens of GPT": total_prompt_tokens,
                          "all messages of GPT": all_message,
                          "code": code,
                          "code run time": total_run_time_code,
                          "predicted load": predicted_load,
                          "n iterations": n_iterations}
                
                return 1, result                

        else: #Code cannot be implemented by the interpreter (returncode = 1).
            print("----------------------------------")
            print("Sorry! The code cannot run in Python interpreters.")
            print("----------------------------------")
            
            result = {"total response time of GPT": total_response_time,
                      "total completion tokens of GPT": total_completion_tokens,
                      "total prompt tokens of GPT": total_prompt_tokens,
                      "all messages of GPT": all_message,
                      "code": code,
                      "code run time": interpreter_output["run_time_code"],
                      "predicted load": interpreter_output["predicted_load"],
                      "n iterations": None}
            
            return 1, result

def Log(returncode, results):
    """Write the automated model deployment results into a log file of automated model deployment.
    
    Inputs
    ----------
    n_trial: Current number of trial.
    returncode: Return code of automated model deployment.
    results: Results of automated model deployment.
        
    Outputs
    ----------
    n_trial: Next number of trial (n_trial plus 1).
    
    """
    
    with open("Log_AutoModelDeployment.txt", 'a') as f:
        f.write("==========================================\n")
        f.write("Returncode: ")
        f.write(str(returncode))
        f.write("\n")
        f.write("------------------------------------------\n")
        f.write("Total response time of GPT: ")
        f.write(str(results["total response time of GPT"]))        
        f.write("\n")
        f.write("------------------------------------------\n")
        f.write("Total completion tokens of GPT: ")
        f.write(str(results["total completion tokens of GPT"]))        
        f.write("\n")
        f.write("------------------------------------------\n")
        f.write("Total prompt tokens of GPT: ")
        f.write(str(results["total prompt tokens of GPT"]))        
        f.write("\n")
        f.write("------------------------------------------\n")
        f.write("Code run time: ")
        f.write(str(results["code run time"]))
        f.write("\n")
        f.write("------------------------------------------\n")
        f.write("Number of iterations for code correction: ")
        f.write(str(results["n iterations"]))    
        f.write("\n")       
        f.write("------------------------------------------\n")
        f.write("Chats for coding and correction:\n")
        for n in range(len(results["all messages of GPT"])):
            f.write("-------------------\n")
            f.write(f"The {n}th chat:\n")
            for m in range(len(results["all messages of GPT"][n])):
                f.write("Role: ")
                f.write(str(results["all messages of GPT"][n][m]["role"]))
                f.write("\n")
                f.write("Content: ")
                f.write(str(results["all messages of GPT"][n][m]["content"]))
                f.write("\n")
            f.write("\n")
        f.write("==========================================\n")
        f.write("\n")

def LogInitialization():
    """Iniitialize a log file of automated model deployment."""
    
    with open("Log_AutoModelDeployment.txt", 'w') as f:
        f.write("AutoPrediction results:\n")
        f.write("\n")    